# ğŸ“˜ **ETL Pipeline Documentation â€” Apache NiFi â†’ MinIO â†’ Apache Spark (Dockerized)**

This repository contains a fully containerized ETL pipeline that ingests CSV files, converts them to Parquet using Apache NiFi, stores them in MinIO (S3-compatible object store), and transforms them using Apache Spark.
It demonstrates a clean separation of concerns across **ingestion**, **storage**, and **processing** layersâ€”aligned with real-world data engineering practices.

---

# 1ï¸âƒ£ **Prerequisites**

Before running the project, ensure the following are installed on your system:

### âœ” Docker

### âœ” Docker Compose

### âœ” At least 4GB RAM allocated to Docker

### âœ” Stable internet (images + JARs downloaded during build)

---

# 2ï¸âƒ£ **Architecture Overview**

| Component          | Purpose                                                                        |
| ------------------ | ------------------------------------------------------------------------------ |
| **Apache NiFi**    | Ingestion pipeline. Detects CSV files â†’ Converts to Parquet â†’ Uploads to MinIO |
| **MinIO**          | Object storage serving as the projectâ€™s Data Lake                              |
| **Apache Spark**   | Reads Parquet from MinIO â†’ Cleans, normalizes, and transforms data             |
| **Docker Compose** | Orchestrates the entire environment                                            |

**High-level flow:**

```
CSV Files â†’ NiFi â†’ MinIO (raw-zone/) â†’ Spark â†’ MinIO (clean-zone/)
```

---

# 3ï¸âƒ£ **Why These Technologies?**

### â­ **Apache NiFi**

Used because it's a production-grade ingestion tool:

* Visual development (drag-drop flows)
* Automatic file detection, retries, provenance tracking
* Lightweight transformations (CSV â†’ Parquet)
* S3 compatibility via PutS3Object

This avoids tightly coupling ingestion with Spark, giving a more modular architecture.

### â­ **MinIO**

* S3-compatible object storage
* Easy to run locally
* Ideal for data lake patterns
* Works seamlessly with Spark using the S3A connector

### â­ **Apache Spark**

Chosen for:

* Large-scale ETL capability
* Built-in Parquet optimizations
* Efficient handling of semi-structured financial/transactional data
* Ability to run distributed jobs (even with 1-2 workers)

### â­ **Parquet Format**

Parquet is used instead of CSV because:

* Columnar (fast analytical reads)
* Highly compressed
* Schema-aware
* Ideal for Spark processing
* Smaller storage footprint in MinIO

---

# 4ï¸âƒ£ **Dataset Summary (Transactional Credit-Risk Data)**

The CSV represents **loan applications and credit-risk attributes**, such as:

* Demographics (Age, Gender, Marital Status)
* Financial metrics (Income, Assets, Credit Score)
* Loan attributes (Amount, Purpose, Employment Status)
* Behavior (Payment History, Defaults)
* Risk Rating (Low/Medium/High)

Spark later cleans & normalizes these fields to make them analytics-ready.

---

# 5ï¸âƒ£ **Set Up & Run the Environment**

## â–º Step 1: Clone the Repository

```bash
git clone https://github.com/crypticwisdom/nifi-minio-spark-etl.git
cd nifi-minio-spark-etl
```

## â–º Step 2: Build & Start All Services

```bash
docker compose build
docker compose up -d
```

## ğŸ“Œ Build Time

The initial build (Spark image + JAR downloads) takes:

â± **~25â€“30 minutes**
![Docker Build](./assets/docker-build.png)
---

# 6ï¸âƒ£ **Accessing the Services**

| Service             | URL                                            |
| ------------------- | ---------------------------------------------- |
| **NiFi UI**         | [http://localhost:8443](http://localhost:8443) |
| **Spark Master UI** | [http://localhost:8080](http://localhost:8080) |
| **MinIO Console**   | [http://localhost:9001](http://localhost:9001) |

Default MinIO credentials:

```
User: minioadmin
Pass: minioadmin
```

---

# 7ï¸âƒ£ **NiFi Ingestion Pipeline**

## ğŸ“Œ NiFi Flow Design


```
ListFile â†’ FetchFile â†’ ConvertRecord â†’ PutS3Object
```
## Screenshot of NIFI Processors
![NIFI Processing Data ](./assets/nifi-screenshot.png)

---

## ğŸ“˜ **Processor Breakdown**

### âœ” ListFile

Scans `./data/input` (mounted to NiFi as `/data/input`) for new CSVs.

### âœ” FetchFile

Retrieves file content into FlowFile.

### âœ” ConvertRecord

* CSVReader â†’ parses CSV
* ParquetRecordSetWriter â†’ writes Parquet

### âœ” PutS3Object

Writes the Parquet file to MinIO bucket:

```
raw-zone/
```

---

## ğŸ“¸ **Include your NiFi Screenshots here**

* Canvas view
* Processor configuration
* Successful flowfile provenance

---

## ğŸ“¥ **Importing the NiFi Template (.xml)**

If recruiters want the ready-made workflow:

```
NiFi UI â†’ Operate Palette/Right click â†’ select "Upload Template" â†’ Choose CSV-Extraction_ingestion_Layer_.xml from this directory â†’ Add to Canvas
```

The template is located at:

```
nifi/templates/CSV-Extraction_ingestion_Layer_.xml
```

This ensures the ETL flow is reproducible across environments.

---

# 8ï¸âƒ£ **Spark Processing**

You can process the data either:

### âœ” Through NiFi â†’ Parquet â†’ Spark
After NIFI ingest data into minio bucket 'raw-zone.

## â–¶ Enter Spark client container

```bash
docker exec -it pyspark-client bash
```

## â–¶ Run your Spark ETL job

```bash
spark-submit \
  --master spark://spark-master:7077 \
  processor.py
```

OR

### âœ” (Fallback Option) **Directly with Spark** if NiFi isn't available

---

## â–¶ Enter Spark client container

```bash
docker exec -it pyspark-client bash
```

## â–¶ Run your Spark ETL job

```bash
spark-submit \
  --master spark://spark-master:7077 \
  process-csv.py
```

---

## ğŸ“˜ Spark Script (Summary of Logic)

Your Spark job performs:

* Reading Parquet files from MinIO using S3A
* Casting numeric columns properly
* Standardizing text fields (lowercase, trim)
* Handling missing values
* Computing derived fields (encoded risk, cleaned categories)
* Writing cleaned output to MinIO:

```
s3a://clean-zone/
```

---

# 9ï¸âƒ£ **Project Structure**

```
project/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ CSV-Extraction_Ingestion_Layer_.xml
â”œâ”€â”€ spark-scripts/
â”‚   â”œâ”€â”€ Dockerfile
â”‚        â””â”€â”€ processor.py   # Run this for processing store parquet on MINIO
â”‚        â””â”€â”€ process-csv.py # Run this to avoid the NIFI stress, and process csv data directly
â”‚
â”œâ”€â”€ nifi/
â”‚   â””â”€â”€ nifi_data/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input/       # Drop CSV files here
â”‚
â””â”€â”€ minio/
    â””â”€â”€ minio_data/
```

---

# ğŸ”Ÿ **Alternative Path (If NiFi Is Tedious to Configure)**

A fallback PySpark script (`process_csv.py`) is also included.

It:

* Reads raw CSV directly
* Cleans & transforms
* Writes Parquet directly to MinIO

Recruiters can run it without setting up NiFi:

```bash
spark-submit \
  --master spark://spark-master:7077 \
  process-csv.py
```

This ensures the project is **still fully testable even without NiFi running**.

---

# 1ï¸âƒ£1ï¸âƒ£ **Conclusion**

This project demonstrates:

* **Proper separation of ingestion â†’ storage â†’ transformation layers**
* Use of **NiFi** for production-like ingestion flows
* Use of **Parquet** for optimized analytics
* Integration of **Spark** with S3-compatible storage
* Containerized, self-contained environment suitable for local or demo deployments
* A clean, modular ETL implementation that mirrors real-world data engineering pipelines


